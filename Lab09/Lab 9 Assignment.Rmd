---
title: "R Notebook"
output: html_notebook
---

```{r, warning=F, message=F}

rm(list=ls())

require(sf)
require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(PresenceAbsence)
require(ecospat)
#Don't forget to load your other R packages!
```

# This first code chunk just recreates the maps we built in the lab.

```{r}

# Model building data
vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))



# Validation data
vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))



# Bringing in the covariates
elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')


# Resampling to make the covariate rasters match
mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

# Mesic forest within 1 km
probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')


# Building the raster stack
layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')


#Creating background points
set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Extracting covariates for our different points
presCovs = extract(layers, vathPresXy)
backCovs = extract(layers, backXy)
valCovs = extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

presCovs = presCovs[complete.cases(presCovs),]
backCovs = backCovs[complete.cases(backCovs),]

# Combining presence and background data into one dataframe

backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)

# Fitting bioclim envelope model
tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)



# Fitting GLM
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')


# Fitting GAM
gamModel = gam(pres ~ s(canopy, k=6) + s(elev, k=6) + s(mesic1km, k=6) + s(precip, k=6), family='binomial', data=presBackCovs, method='ML')

gamMap = predict(layers, gamModel, type='response')


# Fitting boosted regression tree model

boostModel = gbm(pres ~ elev + canopy + mesic1km + precip, distribution='bernoulli', n.trees=100, interaction.depth=2, shrinkage=0.1, bag.fraction=0.5, data=presBackCovs)

boostMap = predict(layers, boostModel, type='response')
boostMap = mask(boostMap, layers$canopy)


# Fitting random forest model

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)


#Fitting maxent model

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')


maxentMap = predictMaxNet(maxentModel, layers, type='logistic')

```



# Challenge 1 (4 points)

In the lab, we fit 6 SDMs. We then calculated discrimination statistics for all 6 and a calibration plot for 1 of them. Create calibration plots for the remaining 5 models, and then make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best." Defend your answer.

```{r}
# discrimination stats data frame setup
tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]


valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1])

# discrimination stats for all models

summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData[,2], valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData[,i+2]*valData[,2] + (1-valData[,i+2]) * (1-valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData[,i+2] + 0.01)*valData[,2] + log((1-valData[,i+2]))*(1-valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData)[3:8])

summaryEval


```
```{r}
### Calibration

glmcal = calibration.plot(valData, which.model=2, N.bins=20, xlab='predicted', ylab='Observed', main='glm')
glmcal
biocal = calibration.plot(valData, which.model=1, N.bins=20, xlab='predicted', ylab='Observed', main='bio')
biocal
gamcal = calibration.plot(valData, which.model=3, N.bins=20, xlab='predicted', ylab='Observed', main='gam')
gamcal
boostcal = calibration.plot(valData, which.model=4, N.bins=20, xlab='predicted', ylab='Observed', main='boost')
boostcal
RFcal = calibration.plot(valData, which.model=5, N.bins=20, xlab='predicted', ylab='Observed', main='RF')
RFcal
maxentcal = calibration.plot(valData, which.model=6, N.bins=20, xlab='predicted', ylab='Observed', main='maxent')
maxentcal
```


Based on the discrimination stats eval table, glm has the best AUC (.67), as in 67% of all random locations that are presence locations, should have values that are higher than a randomly selected absence location. AUC values are relatively neck and neck for all models other than bio, which is considerably lower than glm, and might not have the best discrepancy for determining presence and absence at random locations. True positive rates were lower than I expected, the highest being boost. Again, nothing is performing exceedingly well over anything else yet. True absence rates are better overall with maxent and glm leading the pack. Depending on if I was more interested in true positives for presence or absence, I might focus more on sensitivity (boost) and specificity (maxent) values alone. For instance, if I'm trying to map an endangered/rare species, my objective might have me focusing more so on sensitivity rather than specificity and TSS (of course I'd be looking at them too though). But if my goal is overall accuracy for both presence and absence, the TSS and kappa would have me leaning towards glm (I wouldn't consider maxent here due to its lack of true intercept, even though its got a slightly better kappa).
  glm also has the best likelihood value of all the presence/background models, and so glm is looking   like the best for overall true positive rates and  being able to differentiate between presence and   absence at random locations. I might go with GAM if I wanted specificity and sensitivity to be on    more equal footing while still being close to glm's stats in other aspects.        
Based on the callibration plots, gam looks like it might be the best afterall, as it seems to be the best at detecting presence in all bins, whereas everything else is under predicting for most bins (not detecting true presence for increasing SDM values)


# Challenge 2 (4 points)

Each SDM we created uses a different algorithm with different assumptions. Because of this, ecologists frequently use "ensemble" approaches that aggregate predictions from multiple models in some way. Here we are going to create an ensemble model by calculating a weighted average of the predicted occupancy values at each pixel. We will calculate weights based on model AUC values to ensure that the models with the best AUC values have the most influence on the predicted values in the ensemble model.

Create a raster stack that combines the glmMap, gamMap, boostMap, and rfMap (hint use c()).

Next, create a vector of the AUC values for each model.

Lastly, use the weighted.mean() function in the terra package to create the new raster as a weighted average of the previous 4 rasters.

Plot the result, and explain why we left out the bioclim and Maxent models for this ensemble model.

```{r}
stackedmodels = c(glmMap, gamMap, boostMap, rfMap)
names(stackedmodels) = c('glmVal', 'gamVal', 'boostVal', 'rfVal')
stackedmodels

AUCvalues = c(0.6726221, 0.6455923, 0.6403391, 0.6322577)

AUCvalues

ensemble = weighted.mean(stackedmodels, AUCvalues)

```

I believe we left out maxent and bioclim because they were based on presence only data and therfore lack y intercepts that were derived the same way that these models were. So basically they are incomparable in terms of modeling predicted values within in the same raster template. 



# Challenge 3 (4 points)

Is this ensemble model an improvement over one of the models you built previously? Provide evidence and explain the criteria you used to come to your conclusion.

```{r}
# Extract and create new back/pres/val datasets

tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]

newValcovs = valCovs[complete.cases(valCovs),]
newValcovsXy = as.matrix(newValcovs %>% select(EASTING, NORTHING))

ens_valdata = valData[,1:2]
ens_valdata$ensemble = extract(ensemble, newValcovsXy)
ens_valdata
colnames(ens_valdata)[3] = 'ensemble'


# discrimination stats for all models

ens_summaryEval = data.frame(matrix(nrow=0, ncol=9))

ens_nModels = ncol(ens_valdata)-2

for(i in 1:ens_nModels){
  
  #AUC
  auc = auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(ens_valdata, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(ens_valdata, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(ens_valdata, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(ens_valdata, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(ens_valdata[,2], ens_valdata[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(ens_valdata[,i+2]*ens_valdata[,2] + (1-ens_valdata[,i+2]) * (1-ens_valdata[,2])))
  ll = ifelse(ll == '-Inf', sum(log(ens_valdata[,i+2] + 0.01)*ens_valdata[,2] + log((1-ens_valdata[,i+2]))*(1-ens_valdata[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  ens_summaryEval = rbind(ens_summaryEval, summaryI)
}

ens_summaryEval = ens_summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(ens_valdata)[3:4])

ens_summaryEval




```

Answer the question here.



# Challenge 4 (4 points)

In the lab we built models using presence-background data then validated those models with presence-absence data. For this challenge, you're going to compare the predictive ability of a model built using presence-background data with one built using presence-absence data.

Fit a GLM using the presence-background data as we did in the lab (i.e., use the presBackCovs dataframe). Fit a second GLM using the presence-absence data (i.e., use the presAbsCovs dataframe). Validate both of these models on the novel presence-absence data (valCovs dataset). Specifically, calculate and compare AUC, Kappa, and TSS for these two models. Which model does a better job of prediction for the validation data and why do you think that is? 

```{r}
# Fitting GLM w/presabs
glmModel_presabs = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presAbsCovs)

glmMap_presabs = predict(layers, glmModel_presabs, type='response')

# discrimination stats data frame setup
tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]


glm_backVSabs_valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         glm_back_Val = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         glm_abs_Val = predict(glmModel_presabs, tmp %>% select(canopy:precip), type='response'))

# discrimination stats for glm models

summaryEval_glm = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(glm_backVSabs_valData)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(glm_backVSabs_valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(glm_backVSabs_valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(glm_backVSabs_valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(glm_backVSabs_valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(glm_backVSabs_valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(glm_backVSabs_valData[,2], glm_backVSabs_valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(glm_backVSabs_valData[,i+2]*glm_backVSabs_valData[,2] + (1-glm_backVSabs_valData[,i+2]) * (1-glm_backVSabs_valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(glm_backVSabs_valData[,i+2] + 0.01)*glm_backVSabs_valData[,2] + log((1-glm_backVSabs_valData[,i+2]))*(1-glm_backVSabs_valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval_glm = rbind(summaryEval_glm, summaryI)
}

summaryEval_glm = summaryEval_glm %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(glm_backVSabs_valData)[3:4])

summaryEval_glm

```

While some of these metrics are now just slighlty different, the ones that have reacted strongest with the use of presabs data were sensitivity and specificity. The values for each basically flip flopped so that the sensitivity value for the glm with background data resemebles the specificity for the model with absence data and vice versa. This also explains why the TSS is just barely different despite the changes in sensitivity and specificity between models. I can only imagine this results from the nature of background data, as it would theoretically attribute to more false positive rates than absence data, where absence is actually known. The high specificity value for the background model is likely an unrealistic inflation when compared to the absence model. Likewise, true positive rates for presence benefit from the presence/absence data, as they're no longer being determined based on an arbitrary random placement. I think this is also evident in the higher AUC score. If I'm interpreting this correctly, a larger proportion of random presence locations are occurring at higher SDM values than random locations of absence locations, so it's doing a better job of differentiating absence from presence. Likelihood and thresholds have barely changed and so there's not much to speak on there.  



# Challenge 5 (4 points)

Now calculate the same statistics (AUC, Kappa, and TSS) for each model you developed in Challenge 4 using K-fold validation with 5 groups. Do these models perform better or worse based on K-fold validation (as compared to validation based on novel data)? Why might that occur?

```{r}
#Place your code here
```

Answer the question here.
